from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForQuestionAnswering
import torch
import re

# Load the models and tokenizers
dialog_model_name = "C:/Users/gauri/PycharmProjects/pythonProject3/DialoGPTmedium"
qa_model_name = "distilbert-base-uncased-distilled-squad"

dialog_tokenizer = AutoTokenizer.from_pretrained(dialog_model_name)
dialog_model = AutoModelForCausalLM.from_pretrained(dialog_model_name)

qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_name)
qa_model = AutoModelForQuestionAnswering.from_pretrained(qa_model_name)

# Ensure the pad token is set to eos token for the dialog model
dialog_tokenizer.pad_token = dialog_tokenizer.eos_token


# Function to generate response using DialoGPT
def chatbot_response(user_input, chat_history_ids=None):
    # Encode user input and append to chat history
    new_input_ids = dialog_tokenizer.encode(user_input + dialog_tokenizer.eos_token, return_tensors="pt")
    bot_input_ids = new_input_ids if chat_history_ids is None else torch.cat([chat_history_ids, new_input_ids], dim=-1)

    # Create attention mask
    attention_mask = bot_input_ids.ne(dialog_tokenizer.pad_token_id).int()

    # Generate response with adjusted sampling parameters
    chat_history_ids = dialog_model.generate(
        bot_input_ids,
        max_length=1000,
        pad_token_id=dialog_tokenizer.eos_token_id,
        temperature=0.9,  # Increased temperature for more diversity
        top_p=0.95,  # Top-p sampling for more coherent outputs
        top_k=50,  # Restrict token pool to top k
        do_sample=True,
        attention_mask=attention_mask
    )
    response = dialog_tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)
    return response, chat_history_ids


# Function to generate response using DistilBERT (QA model)
def qa_response(question_input):
    # Predefined knowledge base with more common variations of questions
    knowledge_base = {
        "What is a car?": "A car is a vehicle that is used for transportation. It typically runs on four wheels and is powered by an engine.",
        "Who is the president of the USA?": "The president of the United States is Joe Biden.",
        "What is photosynthesis?": "Photosynthesis is the process used by plants and other organisms to convert light energy into chemical energy.",
        "What is the capital of India?": "The capital of India is New Delhi.",
        "What is the capital of China?": "The capital of China is Beijing.",
        "What is the capital of the USA?": "The capital of the United States is Washington, D.C.",
        "What is the capital of Russia?": "The capital of Russia is Moscow.",
        "Who is the prime minister of India?": "The Prime Minister of India is Narendra Modi.",
        "What is engineering?": "Engineering is the application of science and mathematics to solve problems and design systems, structures, machines, and devices.",
        "How are you?": "I'm an AI, so I don't have feelings, but I'm here to help you!",
        "Tell me a joke?": "Why don't skeletons fight each other? They don't have the guts!",
        "Who wrote Romeo and Juliet?": "Romeo and Juliet was written by William Shakespeare.",
    }

    # Normalize and clean user input for matching
    normalized_input = re.sub(r'[^\w\s]', '', question_input.lower())  # Remove punctuation

    # Check if the question matches any entry in the knowledge base (case-insensitive)
    for question, answer in knowledge_base.items():
        normalized_question = re.sub(r'[^\w\s]', '', question.lower())  # Clean punctuation from knowledge base
        if normalized_input in normalized_question:
            return answer

    # If the question is not in the knowledge base, use the QA model
    context = "Please provide more context or specify your question further."
    inputs = qa_tokenizer.encode_plus(question_input, context, add_special_tokens=True, return_tensors="pt")
    
    # Forward pass through the model
    outputs = qa_model(**inputs)

    # Extract start and end logits (not scores)
    answer_start_scores = outputs.start_logits
    answer_end_scores = outputs.end_logits

    # Get the start and end token positions
    answer_start = torch.argmax(answer_start_scores)
    answer_end = torch.argmax(answer_end_scores) + 1  # Add 1 because end is inclusive

    # Convert token IDs to text
    answer = qa_tokenizer.convert_tokens_to_string(
        qa_tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end])
    )
    
    # If no valid answer is found, return "I am still learning"
    if not answer.strip():
        return "I am still learning."

    return answer


if __name__ == "__main__":
    print("Chatbot ready! Type 'exit' to quit.")
    chat_history = None
    while True:
        try:
            user_input = input("You: ")

            # If the user input is a question, use the QA model
            if '?' in user_input:
                answer = qa_response(user_input)  # Here, 'user_input' is used
                print(f"Chatbot (QA): {answer}")

            # Otherwise, use the DialoGPT model for conversation
            else:
                if user_input.lower() == "exit":
                    print("Goodbye!")
                    break
                reply, chat_history = chatbot_response(user_input, chat_history)
                print(f"Chatbot: {reply}")

        except KeyboardInterrupt:
            print("\nGoodbye!")
            break
